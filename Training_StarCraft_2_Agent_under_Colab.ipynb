{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training StarCraft 2 Agent under Colab",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8V2E13xR5nX",
        "colab_type": "text"
      },
      "source": [
        "# How to train StarCraft II Bots\n",
        "\n",
        "In this notebook, we will get started with StarCraft II Machine Learning on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwYYNIbITWgr",
        "colab_type": "text"
      },
      "source": [
        "# Prerequisite steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLOlLbH2R7Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment the line below to use dev branch of pysc2\n",
        "#!pip install git+https://github.com/deepmind/pysc2.git@dev\n",
        "\n",
        "# Note: Colab does not have an X Server, installing a virtual one\n",
        "!pip install -q pysc2 pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl mesa-utils libosmesa6-dev xorg x11-xserver-utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5NEmiwgSLR5",
        "colab_type": "text"
      },
      "source": [
        "## Download StarCraft II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX0JoA3dZB2h",
        "colab_type": "text"
      },
      "source": [
        "Note: By typing in the password ‘iagreetotheeula’ you agree to be bound by the terms of Blizzard's [AI and Machine Learning License](http://blzdistsc2-a.akamaihd.net/AI_AND_MACHINE_LEARNING_LICENSE.html)\n",
        "\n",
        "Blizzard's CDNs are not very fast, so if you have space free in your Google Drive, I highly recommend uploading StarCraft II onto Google Drive and download from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9viW3ESbSNhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://blzdistsc2-a.akamaihd.net/Linux/SC2.4.0.2.zip\n",
        "!unzip -P iagreetotheeula -oq SC2.4.0.2.zip -d ~"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvMu0KmvSF8j",
        "colab_type": "text"
      },
      "source": [
        "## Getting the maps\n",
        "\n",
        "Like StarCraft II itself, I recommend downloading all the maps and uploading it to Google Drive for it to download faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRk95lbLSFL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/deepmind/pysc2/releases/download/v1.0/mini_games.zip\n",
        "!unzip -P iagreetotheeula -oq mini_games.zip -d ~/StarCraftII/Maps/\n",
        "\n",
        "map_packs = [\"Ladder2017Season1.zip\", \"Ladder2017Season2.zip\", \"Ladder2017Season3_Updated.zip\", \"Ladder2017Season4.zip\", \"Ladder2018Season1.zip\", \"Melee.zip\"]\n",
        "\n",
        "for file in map_packs:\n",
        "    !wget https://blzdistsc2-a.akamaihd.net/MapPacks/{file}\n",
        "    !unzip -P iagreetotheeula -oq {file} -d ~/StarCraftII/Maps/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckLzQaqYrHO8",
        "colab_type": "text"
      },
      "source": [
        "## Remove TCMalloc\n",
        "\n",
        "This is the main roadblock stopping us originally from using Google's free GPUS\n",
        "\n",
        "Note that you will get a lot of errors looking like this\n",
        "\n",
        "\n",
        "```\n",
        "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
        "```\n",
        "This is normal, since we had to get rid of TCMalloc to run StarCraft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucy1pZnzjYg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Important - remove libtcmalloc\n",
        "!apt-get remove libtcmalloc*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR_BrGryTM5q",
        "colab_type": "text"
      },
      "source": [
        "# Now we can start Machine Learning\n",
        "\n",
        "This is an example applying NaiveDQN to a PySC2 Agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zthxww_IBxjZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ_qv6HUBz1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "import math\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from pysc2.agents import base_agent\n",
        "from pysc2.env import sc2_env\n",
        "from pysc2.lib import actions, features, units\n",
        "from absl import app"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehj0EAv7CD-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NaiveMultiLayerPerceptron(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 output_dim: int,\n",
        "                 num_neurons: list = [64, 32],\n",
        "                 hidden_act_func: str = 'ReLU',\n",
        "                 out_act_func: str = 'Identity'):\n",
        "        super(NaiveMultiLayerPerceptron, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_neurons = num_neurons\n",
        "        self.hidden_act_func = getattr(nn, hidden_act_func)()\n",
        "        self.out_act_func = getattr(nn, out_act_func)()\n",
        "\n",
        "        input_dims = [input_dim] + num_neurons\n",
        "        output_dims = num_neurons + [output_dim]\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
        "            is_last = True if i == len(input_dims) - 1 else False\n",
        "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
        "            if is_last:\n",
        "                self.layers.append(self.out_act_func)\n",
        "            else:\n",
        "                self.layers.append(self.hidden_act_func)\n",
        "\n",
        "    def forward(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer(xs)\n",
        "        return xs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj_TqxQmCNAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NaiveDQN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 qnet: nn.Module,\n",
        "                 lr: float,\n",
        "                 gamma: float,\n",
        "                 epsilon: float):\n",
        "        super(NaiveDQN, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.qnet = qnet\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
        "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
        "\n",
        "        self.criteria = nn.MSELoss()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        qs = self.qnet(state)  # Notice that qs is 2d tensor [batch x action]\n",
        "\n",
        "        if self.train:  # epsilon-greedy policy\n",
        "            prob = np.random.uniform(0.0, 1.0, 1)\n",
        "            if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
        "                action = np.random.choice(range(self.action_dim))\n",
        "            else:  # greedy\n",
        "                action = qs.argmax(dim=-1)\n",
        "        else:  # greedy policy\n",
        "            action = qs.argmax(dim=-1)\n",
        "        return int(action)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        s, a, r, ns = state, action, reward, next_state\n",
        "        # Q-Learning target\n",
        "        q_max, _ = self.qnet(next_state).max(dim=-1)\n",
        "        q_target = r + self.gamma * q_max * (1 - done)\n",
        "\n",
        "        # Don't forget to detach `td_target` from the computational graph\n",
        "        q_target = q_target.detach()\n",
        "\n",
        "        # Or you can follow a better practice as follows:\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            q_max, _ = self.qnet(next_state).max(dim=-1)\n",
        "            q_target = r + self.gamma * q_max * (1 - done)\n",
        "        \"\"\"\n",
        "\n",
        "        loss = self.criteria(self.qnet(s)[0, action], q_target)\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlcHcXAoCUNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NaiveDQN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 qnet: nn.Module,\n",
        "                 lr: float,\n",
        "                 gamma: float,\n",
        "                 epsilon: float):\n",
        "        super(NaiveDQN, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.qnet = qnet\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
        "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
        "\n",
        "        self.criteria = nn.MSELoss()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        qs = self.qnet(state)  # Notice that qs is 2d tensor [batch x action]\n",
        "\n",
        "        if self.train:  # epsilon-greedy policy\n",
        "            prob = np.random.uniform(0.0, 1.0, 1)\n",
        "            if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
        "                action = np.random.choice(range(self.action_dim))\n",
        "            else:  # greedy\n",
        "                action = qs.argmax(dim=-1)\n",
        "        else:  # greedy policy\n",
        "            action = qs.argmax(dim=-1)\n",
        "        return int(action)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        s, a, r, ns = state, action, reward, next_state\n",
        "        # Q-Learning target\n",
        "        q_max, _ = self.qnet(next_state).max(dim=-1)\n",
        "        q_target = r + self.gamma * q_max * (1 - done)\n",
        "\n",
        "        # Don't forget to detach `td_target` from the computational graph\n",
        "        q_target = q_target.detach()\n",
        "\n",
        "        # Or you can follow a better practice as follows:\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            q_max, _ = self.qnet(next_state).max(dim=-1)\n",
        "            q_target = r + self.gamma * q_max * (1 - done)\n",
        "        \"\"\"\n",
        "\n",
        "        loss = self.criteria(self.qnet(s)[0, action], q_target)\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS4Osrj5CVyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
        "    actions = (\"do_nothing\",\n",
        "               \"harvest_minerals\",\n",
        "               \"build_supply_depot\",\n",
        "               \"build_barracks\",\n",
        "               \"train_marine\",\n",
        "               \"attack\")\n",
        "\n",
        "    def get_my_units_by_type(self, obs, unit_type):\n",
        "        return [unit for unit in obs.observation.raw_units\n",
        "                if unit.unit_type == unit_type\n",
        "                and unit.alliance == features.PlayerRelative.SELF]\n",
        "\n",
        "    def get_enemy_units_by_type(self, obs, unit_type):\n",
        "        return [unit for unit in obs.observation.raw_units\n",
        "                if unit.unit_type == unit_type\n",
        "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
        "\n",
        "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
        "        return [unit for unit in obs.observation.raw_units\n",
        "                if unit.unit_type == unit_type\n",
        "                and unit.build_progress == 100\n",
        "                and unit.alliance == features.PlayerRelative.SELF]\n",
        "\n",
        "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
        "        return [unit for unit in obs.observation.raw_units\n",
        "                if unit.unit_type == unit_type\n",
        "                and unit.build_progress == 100\n",
        "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
        "\n",
        "    def get_distances(self, obs, units, xy):\n",
        "        units_xy = [(unit.x, unit.y) for unit in units]\n",
        "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
        "\n",
        "    def step(self, obs):\n",
        "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
        "        if obs.first():\n",
        "            command_center = self.get_my_units_by_type(\n",
        "                obs, units.Terran.CommandCenter)[0]\n",
        "            self.base_top_left = (command_center.x < 32)\n",
        "\n",
        "    def do_nothing(self, obs):\n",
        "        return actions.RAW_FUNCTIONS.no_op()\n",
        "\n",
        "    def harvest_minerals(self, obs):\n",
        "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
        "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
        "        if len(idle_scvs) > 0:\n",
        "            mineral_patches = [unit for unit in obs.observation.raw_units\n",
        "                               if unit.unit_type in [\n",
        "                                   units.Neutral.BattleStationMineralField,\n",
        "                                   units.Neutral.BattleStationMineralField750,\n",
        "                                   units.Neutral.LabMineralField,\n",
        "                                   units.Neutral.LabMineralField750,\n",
        "                                   units.Neutral.MineralField,\n",
        "                                   units.Neutral.MineralField750,\n",
        "                                   units.Neutral.PurifierMineralField,\n",
        "                                   units.Neutral.PurifierMineralField750,\n",
        "                                   units.Neutral.PurifierRichMineralField,\n",
        "                                   units.Neutral.PurifierRichMineralField750,\n",
        "                                   units.Neutral.RichMineralField,\n",
        "                                   units.Neutral.RichMineralField750\n",
        "                               ]]\n",
        "            scv = random.choice(idle_scvs)\n",
        "            distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
        "            mineral_patch = mineral_patches[np.argmin(distances)]\n",
        "            return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
        "                \"now\", scv.tag, mineral_patch.tag)\n",
        "        return actions.RAW_FUNCTIONS.no_op()\n",
        "\n",
        "    def build_supply_depot(self, obs):\n",
        "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
        "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
        "        if (len(supply_depots) == 0 and obs.observation.player.minerals >= 100 and\n",
        "                len(scvs) > 0):\n",
        "            supply_depot_xy = (22, 26) if self.base_top_left else (35, 42)\n",
        "            distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
        "            scv = scvs[np.argmin(distances)]\n",
        "            return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
        "                \"now\", scv.tag, supply_depot_xy)\n",
        "        return actions.RAW_FUNCTIONS.no_op()\n",
        "\n",
        "    def build_barracks(self, obs):\n",
        "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
        "            obs, units.Terran.SupplyDepot)\n",
        "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
        "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
        "        if (len(completed_supply_depots) > 0 and len(barrackses) == 0 and\n",
        "                obs.observation.player.minerals >= 150 and len(scvs) > 0):\n",
        "            barracks_xy = (22, 21) if self.base_top_left else (35, 45)\n",
        "            distances = self.get_distances(obs, scvs, barracks_xy)\n",
        "            scv = scvs[np.argmin(distances)]\n",
        "            return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
        "                \"now\", scv.tag, barracks_xy)\n",
        "        return actions.RAW_FUNCTIONS.no_op()\n",
        "\n",
        "    def train_marine(self, obs):\n",
        "        completed_barrackses = self.get_my_completed_units_by_type(\n",
        "            obs, units.Terran.Barracks)\n",
        "        free_supply = (obs.observation.player.food_cap -\n",
        "                       obs.observation.player.food_used)\n",
        "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
        "                and free_supply > 0):\n",
        "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
        "            if barracks.order_length < 5:\n",
        "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
        "        return actions.RAW_FUNCTIONS.no_op()\n",
        "\n",
        "    def attack(self, obs):\n",
        "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
        "        if len(marines) > 0:\n",
        "            attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
        "            distances = self.get_distances(obs, marines, attack_xy)\n",
        "            marine = marines[np.argmax(distances)]\n",
        "            x_offset = random.randint(-4, 4)\n",
        "            y_offset = random.randint(-4, 4)\n",
        "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
        "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
        "        return actions.RAW_FUNCTIONS.no_op()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l7hAzthCofJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t35XWVbCc8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
        "    def step(self, obs):\n",
        "        super(TerranRandomAgent, self).step(obs)\n",
        "        action = random.choice(self.actions)\n",
        "        return getattr(self, action)(obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxwZgLkECmlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_dim = 21\n",
        "a_dim = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVmZ7ZxtCsPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EMAMeter:\n",
        "\n",
        "    def __init__(self,\n",
        "                 alpha: float = 0.5):\n",
        "        self.s = None\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def update(self, y):\n",
        "        if self.s is None:\n",
        "            self.s = y\n",
        "        else:\n",
        "            self.s = self.alpha * y + (1 - self.alpha) * self.s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ZqigqbCuk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
        "    def __init__(self):\n",
        "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
        "        self.qnetwork = NaiveMultiLayerPerceptron(input_dim=s_dim,\n",
        "                           output_dim=a_dim,\n",
        "                           num_neurons=[128],\n",
        "                           hidden_act_func='ReLU',\n",
        "                           out_act_func='Identity')\n",
        "\n",
        "        self.dqn = NaiveDQN(state_dim=s_dim,\n",
        "                             action_dim=a_dim,\n",
        "                             qnet=self.qnetwork,\n",
        "                             lr=1e-4,\n",
        "                             gamma=1.0,\n",
        "                             epsilon=1.0)\n",
        "        \n",
        "        self.print_every = 50\n",
        "        self.ema_factor = 0.5\n",
        "        self.ema = EMAMeter(self.ema_factor)\n",
        "        self.cum_reward = 0\n",
        "        self.episode_count = 0\n",
        "        \n",
        "        self.new_game()\n",
        "        self.data_file = 'rlagent_with_naive_dqn'\n",
        "        if os.path.isfile(self.data_file + '.pt'):\n",
        "            self.dqn.load_state_dict(torch.load(self.data_file + '.pt'))\n",
        "\n",
        "    def reset(self):\n",
        "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
        "        self.new_game()\n",
        "\n",
        "    def new_game(self):\n",
        "        self.base_top_left = None\n",
        "        self.previous_state = None\n",
        "        self.previous_action = None\n",
        "        self.cum_reward = 0\n",
        "\n",
        "    def get_state(self, obs):\n",
        "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
        "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
        "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
        "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
        "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
        "            obs, units.Terran.SupplyDepot)\n",
        "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
        "        completed_barrackses = self.get_my_completed_units_by_type(\n",
        "            obs, units.Terran.Barracks)\n",
        "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
        "\n",
        "        queued_marines = (completed_barrackses[0].order_length\n",
        "        if len(completed_barrackses) > 0 else 0)\n",
        "\n",
        "        free_supply = (obs.observation.player.food_cap -\n",
        "                       obs.observation.player.food_used)\n",
        "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
        "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
        "        can_afford_marine = obs.observation.player.minerals >= 100\n",
        "\n",
        "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
        "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
        "        enemy_command_centers = self.get_enemy_units_by_type(\n",
        "            obs, units.Terran.CommandCenter)\n",
        "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
        "            obs, units.Terran.SupplyDepot)\n",
        "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
        "            obs, units.Terran.SupplyDepot)\n",
        "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
        "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
        "            obs, units.Terran.Barracks)\n",
        "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
        "\n",
        "        return (len(command_centers),\n",
        "                len(scvs),\n",
        "                len(idle_scvs),\n",
        "                len(supply_depots),\n",
        "                len(completed_supply_depots),\n",
        "                len(barrackses),\n",
        "                len(completed_barrackses),\n",
        "                len(marines),\n",
        "                queued_marines,\n",
        "                free_supply,\n",
        "                can_afford_supply_depot,\n",
        "                can_afford_barracks,\n",
        "                can_afford_marine,\n",
        "                len(enemy_command_centers),\n",
        "                len(enemy_scvs),\n",
        "                len(enemy_idle_scvs),\n",
        "                len(enemy_supply_depots),\n",
        "                len(enemy_completed_supply_depots),\n",
        "                len(enemy_barrackses),\n",
        "                len(enemy_completed_barrackses),\n",
        "                len(enemy_marines))\n",
        "\n",
        "    def step(self, obs):\n",
        "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
        "        \n",
        "        #time.sleep(0.5)\n",
        "        \n",
        "        state = self.get_state(obs)\n",
        "        state = torch.tensor(state).float().view(1, 21)\n",
        "        action_idx = self.dqn.choose_action(state)\n",
        "        action = self.actions[action_idx]\n",
        "        done = True if obs.last() else False\n",
        "        if self.previous_action is not None:\n",
        "            self.dqn.learn(self.previous_state,\n",
        "                              self.previous_action,\n",
        "                              obs.reward,\n",
        "                              state,\n",
        "                              done\n",
        "                              )\n",
        "        self.cum_reward += obs.reward\n",
        "        self.previous_state = state\n",
        "        self.previous_action = action_idx\n",
        "        \n",
        "        if obs.last():\n",
        "            self.episode_count = self.episode_count + 1\n",
        "            torch.save(self.dqn.state_dict(), self.data_file + '.pt')\n",
        "            \n",
        "            self.ema.update(self.cum_reward)\n",
        "\n",
        "            if self.episode_count % self.print_every == 0:\n",
        "                print(\"Episode {} || EMA: {} || EPS : {}\".format(self.episode_count, self.ema.s, self.dqn.epsilon))\n",
        "\n",
        "            if self.episode_count >= 150:\n",
        "                self.dqn.epsilon *= 0.999\n",
        "\n",
        "        return getattr(self, action)(obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0FxrDQgTMW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
        "# This does not play well with jupyter notebook\n",
        "# So we will need to monkeypatch sys.argv\n",
        "\n",
        "\n",
        "import sys\n",
        "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
        "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
        "\n",
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Run an agent.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import importlib\n",
        "import threading\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from future.builtins import range  # pylint: disable=redefined-builtin\n",
        "\n",
        "from pysc2 import maps\n",
        "from pysc2.env import available_actions_printer\n",
        "from pysc2.env import run_loop\n",
        "from pysc2.env import sc2_env\n",
        "from pysc2.lib import point_flag\n",
        "from pysc2.lib import stopwatch\n",
        "from pysc2.lib import actions\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# because of Abseil's horrible design for running code underneath Colabs\n",
        "# We have to pull out this ugly hack from the hat\n",
        "if \"flags_defined\" not in globals():\n",
        "    flags.DEFINE_bool(\"render\", True, \"Whether to render with pygame.\")\n",
        "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
        "                            \"Resolution for screen feature layers.\")\n",
        "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
        "                            \"Resolution for minimap feature layers.\")\n",
        "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
        "                            \"Resolution for rendered screen.\")\n",
        "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
        "                            \"Resolution for rendered minimap.\")\n",
        "    flags.DEFINE_enum(\"action_space\", \"RAW\", sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
        "                      \"Which action space to use. Needed if you take both feature \"\n",
        "                      \"and rgb observations.\")\n",
        "    flags.DEFINE_bool(\"use_feature_units\", False,\n",
        "                      \"Whether to include feature units.\")\n",
        "    flags.DEFINE_bool(\"use_raw_units\", True,\n",
        "                      \"Whether to include raw units.\")\n",
        "    flags.DEFINE_integer(\"raw_resolution\", 64, \"Raw Resolution.\")\n",
        "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
        "\n",
        "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
        "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
        "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
        "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
        "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
        "\n",
        "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
        "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
        "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
        "    #                  \"Agent 1's race.\")\n",
        "    flags.DEFINE_string(\"agent\", \"TerranRLAgentWithRawActsAndRawObs\",\n",
        "                        \"Which agent to run, as a python path to an Agent class.\")\n",
        "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
        "                      \"Agent 1's race.\")\n",
        "\n",
        "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
        "    flags.DEFINE_enum(\"agent2_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
        "                      \"Agent 2's race.\")\n",
        "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
        "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
        "\n",
        "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
        "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
        "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
        "\n",
        "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
        "\n",
        "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
        "    flags.mark_flag_as_required(\"map\")\n",
        "\n",
        "flags_defined = True\n",
        "\n",
        "def run_thread(agent_classes, players, map_name, visualize):\n",
        "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
        "  with sc2_env.SC2Env(\n",
        "      map_name=map_name,\n",
        "      players=players,\n",
        "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
        "        feature_screen=FLAGS.feature_screen_size,\n",
        "        feature_minimap=FLAGS.feature_minimap_size,\n",
        "        rgb_screen=FLAGS.rgb_screen_size,\n",
        "        rgb_minimap=FLAGS.rgb_minimap_size,\n",
        "        action_space=FLAGS.action_space,\n",
        "        use_raw_units=FLAGS.use_raw_units,\n",
        "        raw_resolution=FLAGS.raw_resolution),\n",
        "      step_mul=FLAGS.step_mul,\n",
        "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
        "      disable_fog=FLAGS.disable_fog,\n",
        "      visualize=visualize) as env:\n",
        "    #env = available_actions_printer.AvailableActionsPrinter(env)\n",
        "    agents = [agent_cls() for agent_cls in agent_classes]\n",
        "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
        "    if FLAGS.save_replay:\n",
        "      env.save_replay(agent_classes[0].__name__)\n",
        "\n",
        "def main(unused_argv):\n",
        "  \"\"\"Run an agent.\"\"\"\n",
        "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
        "  #stopwatch.sw.trace = FLAGS.trace\n",
        "\n",
        "  map_inst = maps.get(FLAGS.map)\n",
        "\n",
        "  agent_classes = []\n",
        "  players = []\n",
        "\n",
        "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
        "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
        "  #agent_classes.append(agent_cls)\n",
        "  agent_classes.append(TerranRLAgentWithRawActsAndRawObs)\n",
        "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
        "\n",
        "  if map_inst.players >= 2:\n",
        "    if FLAGS.agent2 == \"Bot\":\n",
        "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
        "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
        "    else:\n",
        "      #agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
        "      #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
        "      agent_classes.append(TerranRandomAgent)\n",
        "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
        "\n",
        "  threads = []\n",
        "  for _ in range(FLAGS.parallel - 1):\n",
        "    t = threading.Thread(target=run_thread,\n",
        "                         args=(agent_classes, players, FLAGS.map, False))\n",
        "    threads.append(t)\n",
        "    t.start()\n",
        "\n",
        "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
        "\n",
        "  for t in threads:\n",
        "    t.join()\n",
        "\n",
        "  if FLAGS.profile:\n",
        "    pass\n",
        "    #print(stopwatch.sw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwFx7U8CGC3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  app.run(main)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36uEfbuUGGwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}